{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16282e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "# The data (features) are stored in mnist.data\n",
    "X = mnist.data\n",
    "\n",
    "# The target labels are stored in mnist.target\n",
    "y = mnist.target.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ed84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "random_state = 42\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, train_size=0.7, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b292de74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ea2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, n_in, n_out):\n",
    "\n",
    "        self.W = np.random.normal(0, 1/np.sqrt(n_in), (n_out, n_in))\n",
    "        self.b = np.zeros((n_out, 1))\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return self.W @ self.X + self.b\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        m = dZ.shape[1]\n",
    "        self.dW = (dZ @ self.X.T) / m\n",
    "        self.db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        dX = self.W.T @ dZ\n",
    "        return dX\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.A = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        self.mask = (Z > 0).astype(Z.dtype)\n",
    "        return Z * self.mask\n",
    "\n",
    "    def backward(self, dA):\n",
    "        return dA * self.mask\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.A = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        return dA * (1 - self.A ** 2)\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.A = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        Zs = Z - np.max(Z, axis=0, keepdims=True)\n",
    "        expZ = np.exp(Zs)\n",
    "        return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def one_hot(y, num_classes):\n",
    "    y = np.asarray(y, dtype=np.int64)          # shape (N,)\n",
    "    oh = np.eye(num_classes, dtype=np.float32)[y]  # shape (N, C)\n",
    "    return oh\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_in, n_h1, n_h2, n_out, act=\"relu\"):\n",
    "        self.l1 = Linear(n_in, n_h1)\n",
    "        self.l2 = Linear(n_h1, n_h2)\n",
    "        self.l3 = Linear(n_h2, n_out)\n",
    "        self.act1 = Relu() if act == \"relu\" else Tanh()\n",
    "        self.act2 = Relu() if act == \"relu\" else Tanh()\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        softmax = Softmax()\n",
    "        z1 = self.l1.forward(X)\n",
    "        a1 = self.act1.forward(z1)\n",
    "        z2 = self.l2.forward(a1)\n",
    "        a2 = self.act2.forward(z2)\n",
    "        z3 = self.l3.forward(a2)\n",
    "        a3 = softmax.forward(z3)\n",
    "        self.out = a3\n",
    "\n",
    "    def backward(self, Y):  # Y is correct answer in one_hot form, reshape to (n_h2, 1)\n",
    "        n = Y.shape[0]\n",
    "        dZ3 = self.out - Y\n",
    "        dA3 = self.l3.backward(dZ3)\n",
    "        dZ2 = self.act2.backward(dA3)\n",
    "        dA2 = self.l2.backward(dZ2)\n",
    "        dZ1 = self.act1.backward(dA2)\n",
    "        _ = self.l1.backward(dZ1)\n",
    "\n",
    "    def params_and_grads(self):\n",
    "        return ([self.l1.W, self.l1.b, self.l2.W, self.l2.b, self.l3.W, self.l3.b], [self.l1.dW, self.l1.db, self.l2.dW, self.l2.db, self.l3.dW, self.l3.db])\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, params, lr=1e-3, b1=0.9, b2=0.999, eps=1e-8):\n",
    "        self.lr, self.b1, self.b2, self.eps = lr, b1, b2, eps\n",
    "        self.v = [np.zeros_like(p) for p in params]\n",
    "        self.m = [np.zeros_like(p) for p in params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, params, grads):\n",
    "        self.t += 1\n",
    "        for i, (p, g) in enumerate(zip(params, grads)):\n",
    "            self.v[i] = self.b2 * self.v[i] + (1-self.b2) * g ** 2\n",
    "            self.m[i] = self.b1 * self.m[i] + (1-self.b1) * g\n",
    "            v_hat = self.v[i] / (1 - self.b2 ** self.t)\n",
    "            m_hat = self.m[i] / (1 - self.b1 ** self.t)\n",
    "            p -= self.lr * m_hat/(np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "\n",
    "i = 0\n",
    "model = MLP(784, 10, 10, 10)\n",
    "params, _ = model.params_and_grads()\n",
    "opt = Adam(params)\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "def ce_loss_from_probs(P, Y):  # both (C,N)\n",
    "    eps = 1e-12\n",
    "    return -np.mean(np.log(np.sum(P*Y, axis=0) + eps))\n",
    "\n",
    "\n",
    "class Verbose:\n",
    "    def trial_verbose(i):\n",
    "        print(f\"Training on set {i}\")\n",
    "\n",
    "\n",
    "losses = []\n",
    "for epoch in range(20):\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx = rng.permutation(len(X_train))\n",
    "    for start in range(0, len(X_train), batch_size):\n",
    "        Verbose.trial_verbose(start)\n",
    "        b = idx[start:start+batch_size]\n",
    "        Xb = X_train[b].reshape(-1, 784).T    # (784, B)\n",
    "        yb = y_train[b]\n",
    "        model.forward(Xb)\n",
    "        Yb = one_hot(yb, 10).T                # (10, B)\n",
    "        losses.append(ce_loss_from_probs(model.out, Yb))\n",
    "        model.backward(Yb)\n",
    "        params, grads = model.params_and_grads()\n",
    "        opt.step(params, grads)\n",
    "\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed6472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(set):\n",
    "    for i in set:\n",
    "        model.forward(X_test[i].reshape(-1, 1))\n",
    "        print(f\"Prediction: {numbers[np.argmax(model.out)]}\")\n",
    "        print(f\"Answer: {y_test[i]}\")\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow((X_test[i].reshape(28, 28)),\n",
    "                   cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "\n",
    "test_prediction(np.random.random_integers(1, 21000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes):\n",
    "    y = np.asarray(y, dtype=np.int64)          # shape (N,)\n",
    "    oh = np.eye(num_classes, dtype=np.float32)[y]  # shape (N, C)\n",
    "    return oh\n",
    "\n",
    "\n",
    "one_hot([3, 4, 5], 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
